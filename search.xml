<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[rewrite try_file和proxy_pass]]></title>
    <url>%2F2018%2F06%2F22%2Frewrite-try-file%E5%92%8Cproxy-pass%2F</url>
    <content type="text"><![CDATA[###rewrite syntax: rewrite regex replacement [flag] Context: server, location, if 如果正则表达式（regex）匹配到了请求的URI（request URI），这个URI会被后面的replacement替换 rewrite的定向会根据他们在配置文件中出现的顺序依次执行 通过使用flag可以终止定向后进一步的处理 如果replacement以“http://”, “https://”, or “$scheme”开头，处理将会终止，请求结果会以重定向的形式返回给客户端（client） 如果replacement字符串里有新的request参数，那么之前的参数会附加到其后面，如果要避免这种情况，那就在replacement字符串后面加上“？” 如果正则表达式（regex）里包含“}” or “;”字符，需要用单引号或者双引号把正则表达式引起来 可选的flag参数如下： last 结束当前的请求处理，用替换后的URI重新匹配location； 可理解为重写（rewrite）后，发起了一个新请求，进入server模块，匹配location； 如果重新匹配循环的次数超过10次，nginx会返回500错误； 返回302 http状态码 ； 浏览器地址栏显示重地向后的url break 结束当前的请求处理，使用当前资源，不在执行location里余下的语句； 返回302 http状态码 ； 浏览器地址栏显示重地向后的url redirect 临时跳转，返回302 http状态码； 浏览器地址栏显示重地向后的url permanent 永久跳转，返回301 http状态码； 浏览器地址栏显示重定向后的url proxy_passSyntax: proxy_pass URL;Context: location, if in location, limit_except 不影响浏览器地址栏的url 设置被代理server的协议和地址，URI可选（可以有，也可以没有） 协议可以为http或https 地址可以为域名或者IP，端口可选；eg： proxy_pass http://localhost:8000/uri/; 如果一个域名可以解析到多个地址，那么这些地址会被轮流使用，此外，还可以把一个地址指定为 server group（如：nginx的upstream）, eg: upstream backend { server backend1.example.com weight=5; server backend2.example.com:8080; server unix:/tmp/backend3; server backup1.example.com:8080 backup; server backup2.example.com:8080 backup; } server { location / { proxy_pass http://backend; } } server name， port， URI支持变量的形式，eg： proxy_pass http://$host$uri; 这种情况下，nginx会在server groups（upstream后端server）里搜索server name，如果没有找到，会用dns解析 请求的URI按照下面的规则传给后端server 如果proxy_pass的URL定向里包括URI，那么请求中匹配到location中URI的部分会被proxy_pass后面URL中的URI替换，eg： location /name/ { proxy_pass http://127.0.0.1/remote/; } 请求http://127.0.0.1/name/test.html 会被代理到http://example.com/remote/test.html 如果proxy_pass的URL定向里不包括URI，那么请求中的URI会保持原样传送给后端server，eg： location /name/ { proxy_pass http://127.0.0.1; } 请求http://127.0.0.1/name/test.html 会被代理到http://127.0.0.1/name/test.html 一些情况下，不能确定替换的URI location里是正则表达式，这种情况下，proxy_pass里最好不要有URI 在proxy_pass前面用了rewrite，如下，这种情况下，proxy_pass是无效的，eg： location /name/ { rewrite /name/([^/]+) /users?name=$1 break; proxy_pass http://127.0.0.1; }]]></content>
      <categories>
        <category>Server</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sphinx安装配置]]></title>
    <url>%2F2018%2F06%2F21%2Fsphinx%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Sphinx中文入门指南1.Sphinx简介1.1.Sphinx是什么Sphinx是由俄罗斯人Andrew Aksyonoff开发的一个全文检索引擎。意图为其他应用提供高速、低空间占用、高结果 相关度的全文搜索功能。Sphinx可以非常容易的与SQL数据库和脚本语言集成。当前系统内置MySQL和PostgreSQL 数据库数据源的支持，也支持从标准输入读取特定格式 的XML数据。通过修改源代码，用户可以自行增加新的数据源（例如：其他类型的DBMS 的原生支持） 1.2.Sphinx的特性 高速的建立索引(在当代CPU上，峰值性能可达到10 MB/秒); 高性能的搜索(在2 – 4GB 的文本数据上，平均每次检索响应时间小于0.1秒); 可处理海量数据(目前已知可以处理超过100 GB的文本数据, 在单一CPU的系统上可 处理100 M 文档); 提供了优秀的相关度算法，基于短语相似度和统计（BM25）的复合Ranking方法; 支持分布式搜索; 支持短语搜索 提供文档摘要生成 可作为MySQL的存储引擎提供搜索服务; 支持布尔、短语、词语相似度等多种检索模式; 文档支持多个全文检索字段(最大不超过32个); 文档支持多个额外的属性信息(例如：分组信息，时间戳等); 支持断词; 1.3.Sphinx中文分词中文的全文检索和英文等latin系列不一样，后者是根据空格等特殊字符来断词，而中文是根据语义来分词。目前大多数数据库尚未支持中文全文检索，如Mysql。故，国内出现了一些Mysql的中文全文检索的插件，做的比较好的有hightman的中文分词。Sphinx如果需要对中文进行全文检索，也得需要一些插件来补充。其中我知道的插件有 coreseek 和 sfc 。 Coreseek是现在用的最多的sphinx中文全文检索，它提供了为Sphinx设计的中文分词包LibMMSeg 。并提供了多个系统的二进制发行版，其中有rpm,deb及windows下的二进制包。另外，coreseek也为sphinx贡献了以下事项： GBK编码的数据源支持 采用Chih-Hao Tsai MMSEG算法的中文分词器 中文使用手册（这份中文手册对国内使用sphinx新手——特别是英语不太好的人来说，提供了极大的便利） sfc（sphinx-for-chinese）是由网友happy兄提供的另外一个中文分词插件。其中文词典采用的是xdict。据其介绍,经过测试，目前版本在索引速度上(Linux 测试平台)基本上能够达到索引UTF-8英文的一半，即官方宣称速度的一半。（时间主要是消耗在分词上）。 现提供了与sphinx最新版(sphinx 0.9.10)同步的sphinx-for-chinese-0.9.10-dev-r2006.tar.gz 。此版本增加了sql_attr_string，经过本人的测试。其安装和配置都非常方便。happy兄在分词方面还有另外一个贡献——php-mmseg，这是php对中文分词的一个扩展库。 在此，对以上二位作者谨以最大的敬意 此外，如果你对中文分词不感兴趣。或者说仅需要实现类似sql中like的功能，如： select * from product where prodName like ‘%手机%’。sphinx也不会让你失望，这个或许就是官网对中文的简单实现——直接对字索引。并且搜索速度还不错^_^ 。 本文会对以上三种中文应用进行测试，并以文档的方式记录下来，这也许正是本文档的重点。 2.安装配置实例2.1在GNU/Linux/unix系统上安装Sphinx在mysql上的应用有两种方式：①、采用API调用，如使用PHP、java等的API函数或方法查询。优点是可不必对mysql重新编译，服务端进程“低耦合”，且程序可灵活、方便的调用；缺点是如已有搜索程序的条件下，需修改部分程序。推荐程序员使用。②、使用插件方式（sphinxSE）把sphinx编译成一个mysql插件并使用特定的sql语句进行检索。其特点是，在sql端方便组合，且能直接返回数据给客户端不必二次查询（注）,在程序上仅需要修改对应的sql，但这对使用框架开发的程序很不方便，比如使用了ORM。另外还需要对mysql进行重新编译，且需要mysql-5.1以上版本支持插件存储。系统管理员可使用这种方式二次查询注：到现在发布版本为止——sphinx-0.9.9，sphinx在检索到结果后只能返回记录的ID，而非要查的sql数据，故需要重新根据这些ID再次从数据库中查询，正在开发的sphinx 0.9.10版本已可存储这些文本数据，作者曾试过，性能和存储上的效果都不佳，毕竟还没出正式版 本文采用的是第一种方式 在*nix系统下安装，首先需要以下一些软件支持 软件环境： 操作系统：Centos-5.2 数据库：mysql-5.0.77-3.el5 mysql-devel（如果要使用sphinxSE插件存储请使用mysql-5.1以上版本） 编译软件：gcc gcc-c++ autoconf automake Sphinx ：Sphinx-0.9.9 (最新稳定版 ) 安装： [root@localhost ~]# yum install -y mysql mysql-devel [root@localhost ~]# yum install -y automake autoconf [root@localhost ~]# cd /usr/local/src/ [root@localhost src]# wget http://www.sphinxsearch.com/downloads/sphinx-0.9.9.tar.gz [root@localhost src]# tar zxvf sphinx-0.9.9.tar.gz [root@localhost local]# cd sphinx-0.9.9 [root@localhost sphinx-0.9.9]# ./configure –prefix=/usr/local/sphinx #注意：这里sphinx已经默认支持了mysql [root@localhost sphinx-0.9.9]# make &amp;&amp; make install # 其中的“警告”可以忽略 安装完毕后查看一下/usr/local/sphinx下是否有 三个目录 bin etc var，如有，则安装无误！ 3.配置实例3.1、数据源。这里我们采用 mysql的数据源。具体情况如下： Mysql server：192.168.1.10 Mysql db :test Mysql 表：test.sphinx_article mysql&gt; desc sphinx_article;+———–+———————+——+—–+———+—————-+| Field | Type | Null | Key | Default | Extra |+———–+———————+——+—–+———+—————-+| id | int(11) unsigned | NO | PRI | NULL | auto_increment || title | varchar(255) | NO | | | || cat_id | tinyint(3) unsigned | NO | MUL | | || member_id | int(11) unsigned | NO | MUL | | || content | longtext | NO | | | || created | int(11) | NO | MUL | | |+———–+———————+——+—–+———+—————-+6 rows in set (0.00 sec) 3.2、配置文件 [root@localhost ~]#cd /usr/local/sphinx/etc #进入sphinx的配置文件目录 [root@localhost etc]# cp sphinx.conf.dist sphinx.conf #新建Sphinx配置文件 [root@localhost etc]# vim sphinx.conf #编辑sphinx.conf 具体实例配置文件： 索引源source article_src{type = mysql #####数据源类型sql_host = 192.168.1.10 ######mysql主机sql_user = root ########mysql用户名sql_pass = pwd############mysql密码sql_db = test #########mysql数据库名sql_port= 3306 ###########mysql端口sql_query_pre = SET NAMES UTF8 ###mysql检索编码，特别要注意这点，很多人中文检索不到是数据库的编码是GBK或其他非UTF8sql_query = SELECT id,title,cat_id,member_id,content,created FROM sphinx_article ####### 获取数据的sql #####以下是用来过滤或条件查询的属性############ sql_attr_uint = cat_id ######## 无符号整数属性sql_attr_uint = member_idsql_attr_timestamp = created ############ UNIX时间戳属性 sql_query_info = select * from sphinx_article where id=$id ######### 用于命令界面端(CLI)调用的测试 } 索引index article{source = article_src ####声明索引源path = /usr/local/sphinx/var/data/article #######索引文件存放路径及索引的文件名docinfo = extern ##### 文档信息存储方式mlock = 0 ###缓存数据内存锁定morphology = none #### 形态学（对中文无效）min_word_len = 1 #### 索引的词最小长度charset_type = utf-8 #####数据编码 字符表，注意：如使用这种方式，则sphinx会对中文进行单字切分，即进行字索引，若要使用中文分词，必须使用其他分词插件如 coreseek，sfc charset_table = U+FF10..U+FF19-&gt;0..9, 0..9, U+FF41..U+FF5A-&gt;a..z, U+FF21..U+FF3A-&gt;a..z,\A..Z-&gt;a..z, a..z, U+0149, U+017F, U+0138, U+00DF, U+00FF, U+00C0..U+00D6-&gt;U+00E0..U+00F6,\U+00E0..U+00F6, U+00D8..U+00DE-&gt;U+00F8..U+00FE, U+00F8..U+00FE, U+0100-&gt;U+0101, U+0101,\U+0102-&gt;U+0103, U+0103, U+0104-&gt;U+0105, U+0105, U+0106-&gt;U+0107, U+0107, U+0108-&gt;U+0109,\U+0109, U+010A-&gt;U+010B, U+010B, U+010C-&gt;U+010D, U+010D, U+010E-&gt;U+010F, U+010F,\U+0110-&gt;U+0111, U+0111, U+0112-&gt;U+0113, U+0113, U+0114-&gt;U+0115, U+0115, \U+0116-&gt;U+0117,U+0117, U+0118-&gt;U+0119, U+0119, U+011A-&gt;U+011B, U+011B, U+011C-&gt;U+011D,\U+011D,U+011E-&gt;U+011F, U+011F, U+0130-&gt;U+0131, U+0131, U+0132-&gt;U+0133, U+0133, \U+0134-&gt;U+0135,U+0135, U+0136-&gt;U+0137, U+0137, U+0139-&gt;U+013A, U+013A, U+013B-&gt;U+013C, \U+013C,U+013D-&gt;U+013E, U+013E, U+013F-&gt;U+0140, U+0140, U+0141-&gt;U+0142, U+0142, \U+0143-&gt;U+0144,U+0144, U+0145-&gt;U+0146, U+0146, U+0147-&gt;U+0148, U+0148, U+014A-&gt;U+014B, \U+014B,U+014C-&gt;U+014D, U+014D, U+014E-&gt;U+014F, U+014F, U+0150-&gt;U+0151, U+0151, \U+0152-&gt;U+0153,U+0153, U+0154-&gt;U+0155, U+0155, U+0156-&gt;U+0157, U+0157, U+0158-&gt;U+0159,\U+0159,U+015A-&gt;U+015B, U+015B, U+015C-&gt;U+015D, U+015D, U+015E-&gt;U+015F, U+015F, \U+0160-&gt;U+0161,U+0161, U+0162-&gt;U+0163, U+0163, U+0164-&gt;U+0165, U+0165, U+0166-&gt;U+0167, \U+0167,U+0168-&gt;U+0169, U+0169, U+016A-&gt;U+016B, U+016B, U+016C-&gt;U+016D, U+016D, \U+016E-&gt;U+016F,U+016F, U+0170-&gt;U+0171, U+0171, U+0172-&gt;U+0173, U+0173, U+0174-&gt;U+0175,\U+0175,U+0176-&gt;U+0177, U+0177, U+0178-&gt;U+00FF, U+00FF, U+0179-&gt;U+017A, U+017A, \U+017B-&gt;U+017C,U+017C, U+017D-&gt;U+017E, U+017E, U+0410..U+042F-&gt;U+0430..U+044F, \U+0430..U+044F,U+05D0..U+05EA, U+0531..U+0556-&gt;U+0561..U+0586, U+0561..U+0587, \U+0621..U+063A, U+01B9,U+01BF, U+0640..U+064A, U+0660..U+0669, U+066E, U+066F, \U+0671..U+06D3, U+06F0..U+06FF,U+0904..U+0939, U+0958..U+095F, U+0960..U+0963, \U+0966..U+096F, U+097B..U+097F,U+0985..U+09B9, U+09CE, U+09DC..U+09E3, U+09E6..U+09EF, \U+0A05..U+0A39, U+0A59..U+0A5E,U+0A66..U+0A6F, U+0A85..U+0AB9, U+0AE0..U+0AE3, \U+0AE6..U+0AEF, U+0B05..U+0B39,U+0B5C..U+0B61, U+0B66..U+0B6F, U+0B71, U+0B85..U+0BB9, \U+0BE6..U+0BF2, U+0C05..U+0C39,U+0C66..U+0C6F, U+0C85..U+0CB9, U+0CDE..U+0CE3, \U+0CE6..U+0CEF, U+0D05..U+0D39, U+0D60,U+0D61, U+0D66..U+0D6F, U+0D85..U+0DC6, \U+1900..U+1938, U+1946..U+194F, U+A800..U+A805,U+A807..U+A822, U+0386-&gt;U+03B1, \U+03AC-&gt;U+03B1, U+0388-&gt;U+03B5, U+03AD-&gt;U+03B5,U+0389-&gt;U+03B7, U+03AE-&gt;U+03B7, \U+038A-&gt;U+03B9, U+0390-&gt;U+03B9, U+03AA-&gt;U+03B9,U+03AF-&gt;U+03B9, U+03CA-&gt;U+03B9, \U+038C-&gt;U+03BF, U+03CC-&gt;U+03BF, U+038E-&gt;U+03C5,U+03AB-&gt;U+03C5, U+03B0-&gt;U+03C5, \U+03CB-&gt;U+03C5, U+03CD-&gt;U+03C5, U+038F-&gt;U+03C9,U+03CE-&gt;U+03C9, U+03C2-&gt;U+03C3, \U+0391..U+03A1-&gt;U+03B1..U+03C1,U+03A3..U+03A9-&gt;U+03C3..U+03C9, U+03B1..U+03C1, \U+03C3..U+03C9, U+0E01..U+0E2E,U+0E30..U+0E3A, U+0E40..U+0E45, U+0E47, U+0E50..U+0E59, \U+A000..U+A48F, U+4E00..U+9FBF,U+3400..U+4DBF, U+20000..U+2A6DF, U+F900..U+FAFF, \U+2F800..U+2FA1F, U+2E80..U+2EFF,U+2F00..U+2FDF, U+3100..U+312F, U+31A0..U+31BF, \U+3040..U+309F, U+30A0..U+30FF,U+31F0..U+31FF, U+AC00..U+D7AF, U+1100..U+11FF, \U+3130..U+318F, U+A000..U+A48F,U+A490..U+A4CFmin_prefix_len = 0 #最小前缀min_infix_len = 1 #最小中缀ngram_len = 1 # 对于非字母型数据的长度切割 #加上这个选项，则会对每个中文，英文字词进行分割，速度会慢#ngram_chars = U+4E00..U+9FBF, U+3400..U+4DBF, U+20000..U+2A6DF, U+F900..U+FAFF,\#U+2F800..U+2FA1F, U+2E80..U+2EFF, U+2F00..U+2FDF, U+3100..U+312F, U+31A0..U+31BF,\#U+3040..U+309F, U+30A0..U+30FF, U+31F0..U+31FF, U+AC00..U+D7AF, U+1100..U+11FF,\#U+3130..U+318F, U+A000..U+A48F, U+A490..U+A4CF } ######### 索引器配置 #####indexer{mem_limit = 256M ####### 内存限制} ############ sphinx 服务进程 ########searchd{#listen = 9312 ### 监听端口，在此版本开始，官方已在IANA获得正式授权的9312端口，以前版本默认的是3312 log = /usr/local/sphinx/var/log/searchd.log #### 服务进程日志 ，一旦sphinx出现异常，基本上可以从这里查询有效信息，轮换（rotate）出的问题一般可在此寻到答案query_log = /usr/local/sphinx/var/log/query.log ### 客户端查询日志，笔者注：若欲对一些关键词进行统计，可以分析此日志文件read_timeout = 5 ## 请求超时max_children = 30 ### 同时可执行的最大searchd 进程数pid_file = /usr/local/sphinx/var/log/searchd.pid #######进程ID文件max_matches = 1000 ### 查询结果的最大返回数seamless_rotate = 1 ### 是否支持无缝切换，做增量索引时通常需要} 3.3、建立索引文件[root@localhost sphinx]# bin/indexer -c etc/sphinx.conf article ### 建立索引文件的命令Sphinx 0.9.9-release (r2117)Copyright (c) 2001-2009, Andrew Aksyonoff using config file ‘etc/sphinx.conf’…indexing index ‘article’…collected 1000 docs, 0.2 MBsorted 0.4 Mhits, 99.6% donetotal 1000 docs, 210559 bytestotal 3.585 sec, 58723 bytes/sec, 278.89 docs/sectotal 2 reads, 0.031 sec, 1428.8 kb/call avg, 15.6 msec/call avgtotal 11 writes, 0.032 sec, 671.6 kb/call avg, 2.9 msec/call avg[root@localhost sphinx]#出现以上代表已经索引成功，若不成功的话请根据提示的错误修改配置文件，或到这里提问，我看到后会尽快解决 4.应用4.1 在CLI上测试在上一步中，我们建立了索引，现在我们对刚建立的索引进行测试。测试有两种方式：CLI端和API调用 在CLI端上命令测试是使用sphinx自带的搜索命令：search 在article索引上检索 “北京”关键词[root@localhost sphinx]# bin/search -c etc/sphinx.conf 北京Sphinx 0.9.9-release (r2117)Copyright (c) 2001-2009, Andrew Aksyonoff using config file ‘etc/sphinx.conf’…index ‘article’: query ‘北京 ‘: returned 995 matches of 995 total in 0.008 sec displaying matches:1. document=76, weight=2, cat_id=1, member_id=2, created=Sat Jan 23 19:05:09 2010id=76title=??????????cat_id=1member_id=2content=????????????????????????????????created=12642447092. document=85, weight=2, cat_id=1, member_id=2, created=Sat Jan 23 19:05:09 2010id=85title=????????????cat_id=1member_id=2content=??▒????????????▒????????▒????▒?????????????????????????????created=1264244709…..这里省略….20. document=17, weight=1, cat_id=1, member_id=2, created=Sat Jan 23 19:05:09 2010id=17title=????????????cat_id=1member_id=2content=??????????????????????????????????????????????????????????created=1264244709 words:1. ‘北京’: 995 documents, 999 hits 至此，可以看到，我们已经检索出所有有关“北京”的信息 注意：这里我使用的是putty的客户端，在客户端编码设置的是utf-8，这个是测试的前提条件 4.2 API调用在本例中，我使用PHP的api来测试，在测试前，先启动sphinx服务进程，并对centos的防火墙做好9312端口的开放 [root@localhost sphinx]# bin/searchd -c etc/sphinx.conf &amp; ### 使sphinx在后台运行[1] 5759[root@localhost sphinx]# Sphinx 0.9.9-release (r2117)Copyright (c) 2001-2009, Andrew Aksyonoff using config file ‘etc/sphinx.conf’…listening on all interfaces, port=9312 [1]+ Done bin/searchd -c etc/sphinx.conf php测试代码： &lt;?phpheader(‘Content-type:text/html;charset=utf-8′);?&gt;&lt;form name=”form1″ method=”get” action=””&gt; &lt;?php$keyword = $_GET[‘keyword’];if (trim($keyword)==”) {die(‘请输入关键词’);}else {echo ‘关键词是：’.$keyword;} require “sphinxapi.php”;$cl = new SphinxClient();$cl-&gt;SetServer(’192.168.1.150′, 9312); //注意这里的主机#$cl-&gt;SetMatchMode(SPH_MATCH_EXTENDED); //使用多字段模式//dump($cl);$index=”article”;$res = $cl-&gt;Query($keyword, $index);$err = $cl-&gt;GetLastError();dump($res);function dump($var){echo ‘’;var_dump($var);echo ‘’;}?&gt; 检索“北京”dump后的结果是如下： array(10) { ["error"]=> string(0) "" ["warning"]=> string(0) "" ["status"]=> int(0) ["fields"]=> array(2) { [0]=> string(5) "title" [1]=> string(7) "content" } ["attrs"]=> array(3) { ["cat_id"]=> int(1) ["member_id"]=> int(1) ["created"]=> int(2) } ["matches"]=> array(20) { [76]=> array(2) { ["weight"]=> string(1) "2" ["attrs"]=> array(3) { ["cat_id"]=> string(1) "1" ["member_id"]=> string(1) "2" ["created"]=> string(10) "1264244709" } } .....这里省略..... [17]=> array(2) { ["weight"]=> string(1) "1" ["attrs"]=> array(3) { ["cat_id"]=> string(1) "1" ["member_id"]=> string(1) "2" ["created"]=> string(10) "1264244709" } } } ["total"]=> string(3) "995" ["total_found"]=> string(3) "995" ["time"]=> string(5) "0.008" ["words"]=> array(1) { ["北京"]=> array(2) { ["docs"]=> string(3) "995" ["hits"]=> string(3) "999" } } } 至此PHP已可调用出结果！ 本文转载自Sphinx中文指南]]></content>
      <categories>
        <category>Server</category>
      </categories>
      <tags>
        <tag>sphinx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx日志request_time 和upstream_response_time区别]]></title>
    <url>%2F2018%2F06%2F20%2Fnginx%E6%97%A5%E5%BF%97request-time-%E5%92%8Cupstream-response-time%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[笔者在根据nginx的accesslog中$request_time进行程序优化时，发现有个接口，直接返回数据，平均的$request_time也比较大。原来$request_time包含了用户数据接收时间，而真正程序的响应时间应该用$upstream_response_time。 下面介绍下2者的差别： 1、request_time官网描述：request processing time in seconds with a milliseconds resolution; time elapsed between the first bytes were read from the client and the log write after the last bytes were sent to the client 。指的就是从接受用户请求的第一个字节到发送完响应数据的时间，即包括接收请求数据时间、程序响应时间、输出响应数据时间。 2、upstream_response_time官网描述：keeps times of responses obtained from upstream servers; times are kept in seconds with a milliseconds resolution. Several response times are separated by commas and colons like addresses in the $upstream_addr variable 是指从Nginx向后端（php-cgi)建立连接开始到接受完数据然后关闭连接为止的时间。 从上面的描述可以看出，$request_time肯定比$upstream_response_time值大，特别是使用POST方式传递参数时，因为Nginx会把request body缓存住，接受完毕后才会把数据一起发给后端。所以如果用户网络较差，或者传递数据较大时，$request_time会比$upstream_response_time大很多。 所以如果使用nginx的accesslog查看php程序中哪些接口比较慢的话，记得在log_format中加入$upstream_response_time。 转载自运维生存时间]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Filebeat 注意事项]]></title>
    <url>%2F2018%2F06%2F20%2Ffilebeat-asked-questions%2F</url>
    <content type="text"><![CDATA[集中式日志分析平台 - ELK Stack - Filebeat 的注意事项&emsp;&emsp;了解 Filebeat 哪些事儿我搞不定，哪些事儿我无法承诺。官方文档是一个非常好的参考：Frequently Asked Questions。 无法从网络 Volumes 读取 Log不建议使用 Filebeat 从网络 Volumes 读取日志文件。 尽可能在主机上安装 Filebeat 并从那里直接发送日志文件。 从网络Volumes 读取文件（尤其是在Windows上）可能会产生意外的副作用。 例如，更改的文件标识符可能导致 Filebeat 再次从头读取日志文件。 Filebeat 无法采集数据这可能是因为 Filebeat 配置不正确或无法将事件发送到 output，解决方案： 确保配置文件指定正在收集的文件的正确路径。 有关详细信息，请参阅 Step 2: Configuring Filebeat。 验证文件是否早于 ignore_older 指定的值。 ignore_older 默认是禁用的，这取决于设置的值。 我们可以通过为 ignore_older 指定不同的值来更改此行为。 确保 Filebeat 能够将事件发送到已配置的 output。 在调试模式下运行 Filebeat 以确定它是否成功发布事件： ./filebeat -c config.yml -e -d &quot;*&quot; 下面说说 ignore_older 这个配置： 如果启用此选项，Filebeat 将忽略在指定时间跨度之前修改的任何文件。 如果我们将日志文件保留很长时间，那么配置ignore_older 尤其有用。 例如，如果要启动 Filebeat，但只想发送最新文件和自上周以来的文件，则可以配置此选项。我们可以使用时间字符串，如 2h（2小时）和 5m（5分钟）。 默认值为0，禁用该设置。 注释掉配置与将其设置为0具有相同的效果。 注意：我们必须设置 ignore_older &gt; close_inactive。 配置项 close_inactive 表示在 harvester 读取某文件最后一行日志之后，持续时间内某文件没有再写入日志，Filebeat 将关闭文件句柄，默认是 5m。 发生 Too many open filesFilebeat 会持续保持着句柄，以便以便它可以近乎实时地读取新的日志行。如果 Filebeat 正在收集大量文件，则打开的文件数可能会成为问题。在大多数环境中，活动更新的文件数较少。应相应设置 close_inactive 配置选项以关闭不再活动的文件。 还有其他配置选项可以用来关闭文件处理程序，但是所有这些配置选项都应该仔细使用，因为它们可能有副作用。选项是： close_renamedclose_removedclose_eofclose_timeoutharvester_limitclose_renamed 和 close_removed 选项在 Windows 上可用于解决与 log rotate 相关的问题。请参阅 the section called “Open file handlers cause issues with Windows file rotation?。 close_eof 选项在包含大量文件且只有很少 entries 的环境中很有用。close_timeout 选项在允许数据丢失且必须对文件句柄关闭非常重视的情况下非常有用。有关更多详细信息，请参阅 Filebeat Prospectors Configuration。 在使用任何配置选项之前，请确保阅读了这些配置选项的文档。 Registry file 过大Filebeat保持每个文件的状态，并将状态保持在 registry_file 中。 Filebeat 重新启动时，文件状态用于继续在先前位置读取文件。 如果每天生成大量的新文件，则 文registry_file 件可能会增长得太大。 要减小 registry_file 的大小，有两个可用的配置选项：clean_removed 和 clean_inactive。 对于不再 touch 并需要 ignore 的旧文件，建议使用 clean_inactive。 对于已经从磁盘删除的旧文件，则使用 clean_removed 选项。 Inode 复用造成数据采集缺失在 Linux 文件系统上，Filebeat 使用 inode 和 device 来标识文件。当文件从磁盘中删除时，可以将 inode 分配给一个新文件。在涉及文件轮换的用例中，如果旧文件被删除，并且之后立即创建新文件，则新文件可以具有与被移除的文件完全相同的 inode。在这种情况下，Filebeat 假定新文件与旧文件相同，并尝试在旧 offset 继续读取，这是不正确的。 默认情况下，永远不会从 registry_file 中删除。要解决 inode 复用问题，建议使用 clean_ * 选项，特别是 clean_inactive，以删除非活动文件的状态。例如，如果文件每 24 小时轮换一次，并且轮转的文件不再更新，可以将ignore_older 设置为48小时，将 clean_inactive 设置为72小时。 对于从磁盘中删除的文件，可以使用 clean_removed 。请注意，每当在扫描期间找不到文件时，clean_removed 会从 registry_file 清除文件状态。如果文件稍后再次显示，则将从头重新发送。 Filebeat 使用了过多 CPUFilebeat 可能被配置为太频繁地扫描文件。 检查 filebeat.yml 配置文件中 scan_frequency的设置。 将scan_frequency 设置为小于1秒可能导致 Filebeat 扫描磁盘过于频繁。 字段在 Kibana 可视化中不可被索引如果最近执行了 loads 或解析自定义结构化日志的操作，则可能需要刷新索引以使字段在 Kibana 中可用。 要刷新索引，请使用刷新API。 例如： 1curl -XPOST &apos;http://localhost:9200/filebeat-2016.08.09/_refresh&apos; Filebeat 总是不发送文件的最后一行Filebeat 使用换行符来检测事件的结束。 如果行被递增地添加到正在收集的文件，则最后一行之后需要换行符，否则Filebeat 将不会读取文件的最后一行。 如何对 Filebeat 限带宽如果需要限制带宽使用，建议在操作系统上配置网络堆栈以执行带宽限制。 例如，以下 Linux 命令通过在端口 5044 上对 TCP 连接设置 50 kbps 的限制来限制 Filebeat 和 Logstash 之间的连接： 1234tc qdisc add dev $DEV root handle 1: htbtc class add dev $DEV parent 1:1 classid 1:10 htb rate 50kbps ceil 50kbpstc filter add dev $DEV parent 1:0 prio 1 protocol ip handle 10 fw flowid 1:10iptables -A OUTPUT -t mangle -p tcp --dport 5044 -j MARK --set-mark 10 使用OS工具执行带宽限制可以更好地控制策略。 例如，可以使用操作系统工具在白天限制带宽，但不能在夜间限制。 或者，可以保留带宽未封顶，但为流量分配低优先级。 常见的 SSL 相关错误和解决方案以下是一些常见的错误和解决方法： x509: cannot validate certificate for &lt;IP address&gt; because it doesn’t contain any IP SANs 这是因为证书仅对 Subject filed 中显示的 hostname 有效。要解决此问题，请尝试以下解决方案： 为主机名创建一个DNS条目，将其映射到服务器的IP。 在/etc/hosts中为主机名创建一个条目。或在 Windows 上添加一个条目到C:\Windows\System32\drivers\etc\hosts。 重新创建服务器证书，并为服务器的IP地址添加 SubjectAltName（SAN）。这使服务器的证书对主机名和IP地址都有效。 getsockopt: no route to host这不是SSL问题。这是一个网络问题。确保两个主机可以通信。 getsockopt：connection refused这不是SSL问题。确保 Logstash 正在运行，并且没有防火墙阻止流量。 No connection could be made because the target machine actively refused it防火墙拒绝连接。检查防火墙是否阻止客户端，网络或目标主机上的流量。 以上。。官方原文：Log input转载于https://www.jianshu.com/p/d53e80e3b0bd]]></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>filebeat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Starting with filebeat]]></title>
    <url>%2F2018%2F06%2F20%2Ffilebeat-learning-one%2F</url>
    <content type="text"></content>
      <categories>
        <category>ELK</category>
      </categories>
      <tags>
        <tag>filebeat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ruby learning one]]></title>
    <url>%2F2018%2F06%2F19%2FRuby-learning-one%2F</url>
    <content type="text"><![CDATA[Ruby介绍&emsp;&emsp;Ruby 是一种面向对象、命令式、函数式、动态的通用编程语言。在20世纪90年代中期由日本计算机科学家松本行弘（Matz）设计并开发。 &emsp;&emsp;遵守BSD许可证和Ruby License[10][注 1]。它的灵感与特性来自于Perl、Smalltalk、Eiffel、Ada以及Lisp语言。由Ruby语言本身还发展出了JRuby（Java平台）、IronRuby（.NET平台）等其他平台的Ruby语言替代品。—来自WIKI&emsp;&emsp;Ruby 是”程序员的最佳朋友”。 &emsp;&emsp;Ruby 的特性与 Smalltalk、Perl 和 Python 类似。Perl、Python 和 Smalltalk 是脚本语言。Smalltalk 是一个真正的面向对象语言。Ruby，与 Smalltalk 一样，是一个完美的面向对象语言。使用 Ruby 的语法比使用 Smalltalk 的语法要容易得多。 语言特点 Ruby 是开源的，在Web 上免费提供，但需要一个许可证。 [5] Ruby 是一种通用的、解释的编程语言。 Ruby 是一种真正的面向对象编程语言。 Ruby 是一种类似于 Python 和 Perl 的服务器端脚本语言。 Ruby 可以用来编写通用网关接口（CGI）脚本。 Ruby 可以被嵌入到超文本标记语言（HTML）。 Ruby 语法简单，这使得新的开发人员能够快速轻松地学习 Ruby。 Ruby 与 C++ 和 Perl 等许多编程语言有着类似的语法。 Ruby 可扩展性强，用 Ruby 编写的大程序易于维护。 Ruby 可用于开发的 Internet 和 Intranet 应用程序。 Ruby 可以安装在 Windows 和 POSIX 环境中。 Ruby 支持许多 GUI 工具，比如 Tcl/Tk、GTK 和 OpenGL。 Ruby 可以很容易地连接到 DB2、MySQL、Oracle 和 Sybase。 Ruby 有丰富的内置函数，可以直接在 Ruby 脚本中使用。 ---来自百度百科 安装Ruby环境：CentOS [root@www ~]# yum install ruby -yLoaded plugins: fastestmirror, langpacksRepository epel is listed more than once in the configurationRepodata is over 2 weeks old. Install yum-cron? Or run: yum makecache fast……….Complete![root@www ~]# ruby -vruby 2.0.0p648 (2015-12-16) [x86_64-linux][root@www ~]#如上所示安装成功 实例脚本式： [root@www ~]# echo “puts ‘hello world’” &gt; test.rb[root@www ~]# ruby test.rbhello world[root@www ~]#交互式： [root@www ~]# irbirb(main):001:0&gt; puts “hello world”hello world=&gt; nilirb(main):002:0&gt; 字符编码#!/usr/bin/ruby -w -- coding: UTF-8 --puts “你好，世界！”; 命令行选项$ ruby [ options ] [.] [ programfile ] [ arguments … ]选项|描述—–|—–-a|与 -n 或 -p 一起使用时，可以打开自动拆分模式(auto split mode)。请查看 -n 和 -p 选项。-c|只检查语法，不执行程序。-C dir|在执行前改变目录（等价于 -X）。-d|启用调试模式（等价于 -debug）。-F pat|指定 pat 作为默认的分离模式（$;）。-e prog|指定 prog 作为程序在命令行中执行。可以指定多个 -e 选项，用来执行多个程序。-h|显示命令行选项的一个概览。-i [ ext]|把文件内容重写为程序输出。原始文件会被加上扩展名 ext 保存下来。如果未指定 ext，原始文件会被删除。-I dir|添加 dir 作为加载库的目录。-K [ kcode]|指定多字节字符集编码。e 或 E 对应 EUC（extended Unix code），s 或 S 对应 SJIS（Shift-JIS），u 或 U 对应 UTF-8，a、A、n 或 N 对应 ASCII。-l|启用自动行尾处理。从输入行取消一个换行符，并向输出行追加一个换行符。-n|把代码放置在一个输入循环中（就像在 while gets; … end 中一样）。-0[ octal]|设置默认的记录分隔符（$/）为八进制。如果未指定 octal 则默认为 \0。-p|把代码放置在一个输入循环中。在每次迭代后输出变量 $_ 的值。-r lib|使用 require 来加载 lib 作为执行前的库。-s|解读程序名称和文件名参数之间的匹配模式 -xxx 的任何参数作为开关，并定义相应的变量。-T [level]|设置安全级别，执行不纯度测试（如果未指定 level，则默认值为 1）。-v|显示版本，并启用冗余模式。-w|启用冗余模式。如果未指定程序文件，则从 STDIN 读取。-x [dir]|删除 #!ruby 行之前的文本。如果指定了 dir，则把目录改变为 dir。-X dir|在执行前改变目录（等价于 -C）。-y|启用解析器调试模式。–copyright|显示版权声明。–debug|启用调试模式（等价于 -d）。–help|显示命令行选项的一个概览（等价于 -h）。–version|显示版本。–verbose|启用冗余模式（等价于 -v）。设置 $VERBOSE 为 true。–yydebug|启用解析器调试模式（等价于 -y）。]]></content>
      <categories>
        <category>Ruby</category>
      </categories>
      <tags>
        <tag>Learn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PM2的安装和使用]]></title>
    <url>%2F2018%2F06%2F19%2FPM2%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[PM2介绍&emsp;&emsp;PM2 is a process manager for the JavaScript runtime Node.js. In 2016, PM2 is ranked as the 82nd most popular JavaScript project on GitHub.&emsp;&emsp;PM2 or Process Manager 2, is an Open Source, production Node.js process manager helping Developers and Devops manage Node.js applications in production environment. In comparison with other process manager like Supervisord, Forever, Systemd, some key features of PM2 are automatic application load balancing, declarative application configuration, deployment system and monitoring.&emsp;&emsp;Started in 2013 by Alexandre Strzelewicz. The code source is hosted on GitHub and installable via Npm (software) “Npm (software)”).说明：本次安装环境为CentOS7上进行 PM2安装1.安装node首先，从Node.js官网下载对应平台的安装程序，网速慢的童鞋请移步国内镜像。 [root@www ~]# tar xf node-v10.3.0-linux-x64.tar.gz [root@www ~]# cd node-v10.3.0-linux-x64/ [root@www node-v10.3.0-linux-x64]# ls bin CHANGELOG.md include lib LICENSE README.md share [root@www node-v10.3.0-linux-x64]# 加入环境变量中： [root@www node-v10.3.0-linux-x64]# echo &quot;export PATH=/usr/local/node-v10.3.0-linux-x64/bin:$PATH&quot; &gt; /etc/profile.d/nodejs.sh [root@www node-v10.3.0-linux-x64]# source /etc/profile [root@www node-v10.3.0-linux-x64]# npm -v 6.1.0 [root@www node-v10.3.0-linux-x64]# 2.安装pm2通过上面node提供的npm进行安装 [root@www ~]# npm install pm2 -g 如图提示说明安装成功 PM2使用start [options] &lt;file|json|stdin|app_name|pm_id...&gt; 后台启动node.js应用 trigger &lt;proc_name&gt; &lt;action_name&gt; [params] 通过此命令触发操作流程？？ deploy &lt;file|environment&gt; 通过环境变量或配置文件发布node.js项目 startOrRestart &lt;json&gt; start or restart JSON file startOrReload &lt;json&gt; start or gracefully reload JSON file pid [app_name] 返回指定app的pid或返回所有的pid startOrGracefulReload &lt;json&gt; start or gracefully reload JSON file stop [options] &lt;id|name|all|json|stdin...&gt; 停止一个进程 (若要重新启动, 执行pm2 restart &lt;app&gt;) restart [options] &lt;id|name|all|json|stdin...&gt; 重启一个进程 scale &lt;app_name&gt; &lt;number&gt; 在集群模式中依赖总数增加或减少集群数量 snapshot snapshot PM2 memory profile &lt;command&gt; CPU的使用情况 reload &lt;name|all&gt; 重新加载进程 (注意这个只针对http/https的应用) gracefulReload &lt;name|all&gt; 优雅地重新加载一个进程. 发送&quot;shutdown&quot; 关闭所有的连接. id &lt;name&gt; 获取app的id delete &lt;name|id|script|all|json|stdin...&gt; 停止次进程并且冲pm2管理列表中删除此进程 sendSignal &lt;signal&gt; &lt;pm2_id|name&gt; 向目标进程发送一个系统signal ping ping pm2的后台进程 - 如果挂了 就重启 updatePM2 使用内存中的PM2更新内存中的 update updatePM2的别名 install|module:install [options] [module|git:/] 安装或者更新模块 (或者是模块的设置) 然后永久运行它 module:update &lt;module|git:/&gt; 更新模块并永久运行 module:generate [app_name] 在当前目录生成一个事例模块 uninstall|module:uninstall &lt;module&gt; 停止并卸载一个模块 publish|module:publish Publish the module you are currently on set [key] [value] sets the specified config &lt;key&gt; &lt;value&gt; multiset &lt;value&gt; multiset eg &quot;key1 val1 key2 val2 get [key] get value for &lt;key&gt; conf [key] [value] get / set module config values config &lt;key&gt; [value] get / set module config values unset &lt;key&gt; clears the specified config &lt;key&gt; report give a full pm2 report for https://github.com/Unitech/pm2/issues link|interact [options] [secret] [public] [name] linking action to keymetrics.io - command can be stop|info|delete|restart unlink linking action to keymetrics.io - command can be stop|info|delete|restart unmonitor [name] unmonitor target process monitor [name] monitor target process open open dashboard in browser register create an account on keymetrics login login to keymetrics and link current PM2 web launch a health API on 0.0.0.0:9615 dump|save dump all processes for resurrecting them later send &lt;pm_id&gt; &lt;line&gt; send stdin to &lt;pm_id&gt; attach &lt;pm_id&gt; [comman] attach stdin/stdout to application identified by &lt;pm_id&gt; resurrect resurrect previously dumped processes unstartup [platform] disable and clear auto startup - [platform]=systemd,upstart,launchd,rcd startup [platform] setup script for pm2 at boot - [platform]=systemd,upstart,launchd,rcd logrotate copy default logrotate configuration ecosystem|init [mode] generate a process conf file. (mode = null or simple) reset &lt;name|id|all&gt; reset counters for process describe &lt;id&gt; describe all parameters of a process id desc &lt;id&gt; (alias) describe all parameters of a process id info &lt;id&gt; (alias) describe all parameters of a process id show &lt;id&gt; (alias) describe all parameters of a process id list|ls list all processes l (alias) list all processes ps (alias) list all processes status (alias) list all processes jlist list all processes in JSON format prettylist print json in a prettified JSON monit launch termcaps monitoring imonit launch legacy termcaps monitoring dashboard|dash launch dashboard with monitoring and logs flush flush logs reloadLogs reload all logs logs [options] [id|name] stream logs file. Default stream all logs kill kill daemon pull &lt;name&gt; [commit_id] updates repository for a given app forward &lt;name&gt; updates repository to the next commit for a given app backward &lt;name&gt; downgrades repository to the previous commit for a given app gc force PM2 to trigger garbage collection deepUpdate performs a deep update of PM2 serve|expose [path] [port] serve a directory over http via port]]></content>
      <categories>
        <category>Server</category>
      </categories>
      <tags>
        <tag>nodejs</tag>
      </tags>
  </entry>
</search>
